Задача состоит в построении модели классификации изображений автомобилей по их фотографиям.  
Основная идея решения: взять предобученую модель и дообучить под нашу задачу (в моем решении выбрана сеть EfficientNetB6, так как она относится к SOTA на ImageNet и не такая большая).  
Данные для обучения модели (kaggle competitions download -c sf-dl-car-classification) решения проводятся в соответствующем соревновании на Kaggle (https://www.kaggle.com/c/sf-dl-car-classification)

Основной ход моего решения заключался в следующем:  
1. Установка и импорт необходимых библиотек, в т.ч. определение основных переменных и создание необходимых папок для сохранения результатов;
2. Проведение краткого EDA, в т.ч. анализ имеющихся изображений;
3. Аугментация данных (использовалась библиотека albumentations) и создание соответствующих генераторов (с помощью библиотеки ImageDataAugmentor) для подачи данных в модель при обучении;  
4. Для создания модели использовалась техника Tranfer-Leaning: как основа загружалась EfficientNetB6 с исключением полносвязных слоев, которые определяют набор вероятностей к каждому классу ImageNet (исключение "головы"). Вместо исключенных слоев достаивались полносвязные слои под нашу задачу.
5. В основе тренировки модели использовалась техника Fine-Tunning: тренировка модели проводилась с постепенным размораживанием весов слоев, доступных для тренировки и состояла из нескольких шагов (step):  

    Step 1 - тренировка весов слоев только для "головы", с неизменными весами EfficientNetB6 (уже после данного этапа точность на тренировочной выборке превышает 50%, на тестовой - превышает 60%, см. файлы Train_Vall_acc_st_1.png и Train_Vall_loss_st_1.png). Так как в дальнейшем веса будут переобучаться при разморозке модели, то на данном этапе было выбрано небольшое количество эпох обучения. Точность на тренировочной выборке оказывается хуже, но к 5 эпохе видно, что точность тестовой выборки перестает улучшаться, а точность тренировочной выборки растет быстрее.  
    
    Step 2-4 - тренировка с постепенной разморозкой весов слоев EfficientNetB6. Step 2: разморозка 1/2 от всех слоев EfficientNetB6, тренировка 10 эпох; Step 3: разморозка 3/4 от всех слоев EfficientNetB6, тренировка 10 эпох; Step 4: разморозка всех слоев EfficientNetB6, тренировка 10 эпох.      
    Результаты по обучению на шагах 2-4:  
    Наилучшая сходимость тренировочиной и тестовой выборок достигается после Step 2 (разморозка 1/2 от всех слоев) на 10 эпохе и составляет чуть больше 90%, далее видно, что проиходит выход на "плато" (см. файлы Train_Vall_acc_st_2.png и Train_Vall_loss_st_2.png). На данном этапе можно попробовать большее количество эпох (30-50 эпох) с постепенным (по расписанию или по условию неувеличения val_accuracy) уменьшением Learning Rate. Но так как время на обучение ограничено количеством времени использования GPU и слои будут размораживаться далее, соответственно обученные веса будут еще изменяться, то было решено не работать в этом направлении.  
    На шаге 3 (step 3) видно (см. файлы Train_Vall_acc_st_3.png и Train_Vall_loss_st_3.png), что после 4 эпохи идет переобучение модели (качество тренировочной выборки растет, а тестовой выходит на плато, рост уменьшается). Возможно для данного шага можно число эпох лучше ограничить 5 эпохами и продолжить разморозку слоев. Точность тестовой выборки достигает чуть меньше 95%, тренировочной выборки достигает чуть меньше 97% (см. файлы Train_Vall_acc_st_3.png и Train_Vall_loss_st_3.png).
    На шаге 4 (step 4) видно (см. файлы Train_Vall_acc_st_4.png и Train_Vall_loss_st_4.png) видно также переобучение модели, но точность тестовой выборки все же улучшается и становится выше 95,5% на 7 эпохе. 
    
    Step 5_1-5_3. На данных шагах для увеличесния точности обучения модели производится увеличение размера подаваемых изображений в 2 раза (с 224х228 до 448х448 точек). Обучение происходит при всех размороженных слоях, но при этом меняется learning rate: Step 5_1 LR=1e-5, 12-15 эпох (на основании последовательного добавления количества эпох, дальнейшее увеличение количества эпох не приводит к точности выше 97,6%. Добавленные шаги: Step 5_2 LR=1e-6, 10 эпох и Step 5_3 LR=1e-7, 10 эпох необходимо проверить на улучшение качества, ввиду ограниченного количества времени не были полноценно проверены (см. пояснение далее).  
    Важно отметить, что при этом время на обучение возрастает примерно в 4,5 раза (примерно с 6 мин до 27 мин) и обучение 10 эпох каждого шага растягивается примерно до 5 часов. Важно отметить, что на Kaggle есть ограничение на работу одной сессии с GPU (9 часов) и прогнать весь ноутбук и сохранить все результаты можно только исключив данные ограничения. Аналогично есть ограничения на использования GPU и в Google Colab.
    
6. Далее для возможного улучшения предсказания качества модели на валидационной выборке использовалась техника Test Time Augmentations, которая основывается на небольших изменениях данных валидационной выборки (аугментация валидационной выборки) и усреднении полученных предсказаний (небольшие изменения могут помочь модели правильно предсказать класс изображения).

Ноутбук с решением и основные результаты работы представлены в данном проекте.


Что еще можно сделать для улучшения модели/доработки проекта:
1. Попробовать другие архитектуры сетей из SOTA на ImageNet позднее B6, дающие бОльшую точность.
2. Поэкспериментировать с архитектурой «головы» (например, добавить еще 1-2 полносвязных слоев).
3. Попробовать больше эпох на 5 этапе обучения (увеличить до 30 эпох с обязательным callback ReduceLROnPlateau с параметрами monitor='val_accuracy', factor=0.2-0.5, patience=3-5).
4. Не пробовал технику управления Learning Rate: Sheduler, но он требует большего времени и требует исключение ограничения на сессию 9 часов.
5. Использовать внешние датасеты для дообучения модели.
6. Обернуть модель в сервис на Flask (чтобы на практике отследить особенности внедрения DL-моделей в продакшн).
