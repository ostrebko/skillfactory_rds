Задача состоит в построении модели классификации изображений автомобилей по их фотографиям. В выборках присутствуют 10 классов: 'Приора', 'Ford Focus', 'Самара', 'ВАЗ-2110', 'Жигули', 'Нива', 'Калина', 'ВАЗ-2109', 'Volkswagen Passat', 'ВАЗ-21099'.  

Основная идея решения: взять предобученую модель и дообучить под задачу классификации автомобиля по изображению (в моем решении выбрана сеть EfficientNetB6, так как она относится к SOTA на ImageNet, имеет хорошее качество и относительно не большая).  

Данные для обучения модели (kaggle competitions download -c sf-dl-car-classification) решения проводятся в соответствующем соревновании на Kaggle (https://www.kaggle.com/c/sf-dl-car-classification). Данные для иных площадок можно скачать из датасета (приведено в файле data_2_load.md).  

Основной ход моего решения заключался в следующем:  
1. Установка и импорт необходимых библиотек, в т.ч. определение основных переменных и создание необходимых папок для сохранения результатов;
2. Проведение краткого EDA, в т.ч. анализ имеющихся изображений;
3. Аугментация данных (использовалась библиотека albumentations) и создание соответствующих генераторов (с помощью библиотеки ImageDataAugmentor) для подачи данных в модель при обучении;  
4. Для создания модели использовалась техника Transfer-Learning: как основа загружалась EfficientNetB6 с исключением полносвязных слоев, которые определяют набор вероятностей к каждому классу ImageNet (исключение "головы"). Вместо исключенных слоев достраивались полносвязные слои под нашу задачу.
5. В основе тренировки модели использовалась техника Fine-Tunning: тренировка модели проводилась с постепенным размораживанием весов слоев, доступных для тренировки и состояла из нескольких шагов (step):  

    Step 1 - тренировка весов слоев только для "головы", с неизменными весами EfficientNetB6 (уже после данного этапа точность на тренировочной выборке превышает 50%, на тестовой - превышает 60%). Так как в дальнейшем веса будут переобучаться при разморозке модели, то на данном этапе было выбрано небольшое количество эпох обучения. Точность на тренировочной выборке оказывается хуже, но к 5 эпохе точность тестовой выборки перестает улучшаться, а точность тренировочной выборки растет быстрее.  
    
    Step 2-4 - тренировка с постепенной разморозкой весов слоев EfficientNetB6. Step 2: разморозка 1/2 от всех слоев EfficientNetB6, тренировка 10 эпох; Step 3: разморозка 3/4 от всех слоев EfficientNetB6, тренировка 10 эпох; Step 4: разморозка всех слоев EfficientNetB6, тренировка 10 эпох.      
    Результаты по обучению на шагах 2-4:  
    Наилучшая сходимость тренировочиной и тестовой выборок достигается после Step 2 (разморозка 1/2 от всех слоев EfficientNetB6) на 10 эпохе и составляет чуть больше 90%. На данном этапе можно попробовать большее количество эпох (30-50 эпох) с постепенным (по расписанию или по условию неувеличения val_accuracy) уменьшением Learning Rate. Но так как время на обучение ограничено количеством времени использования GPU и слои будут размораживаться далее, соответственно обученные веса будут еще изменяться, то было решено не работать в этом направлении.  
    На шаге 3 (step 3) размораживаю 3/4 всех слоев и обучаю 10 эпох.
    На шаге 4 (step 4) размораживаю всех слои base_model (всех слоев EfficientNetB6) и обучаю 10 эпох. 
    
    Шаги 5_1-5_3 (Step 5_1, 5_2, 5_3: На данных шагах для увеличесния точности обучения модели производится увеличение размера подаваемых изображений в 2 раза (с 224х228 до 448х448 точек). Обучение происходит при всех размороженных слоях, но при этом меняется learning rate: Step 5_1 LR=1e-5, 10 эпох, Step 5_2 LR=1e-5, 8 эпох, Step 5_3 LR=1e-6, 10 эпох. Примечание: На Step 5_2 было решено добавить еще 8 эпох без изменения параметров шага 5_1.  
    Важно отметить, что при увеличении картинки в 2 раза, время на обучение возрасло примерно в 3-4 раза и обучение 10 эпох каждого шага растягивается примерно до 5 часов. В связи с тем, что на Kaggle есть ограничение на работу одной сессии с GPU (9 часов) и прогнать весь ноутбук и сохранить все результаты можно только исключив данные ограничения. Аналогично есть ограничения на использования GPU и в Google Colab. Поэтому в данной работе я сохранил, только ноутбук и результаты предсказания на валидационной выборке (submission).
    На шаге 5_3 параметр patience в callback ReduceLROnPlateau , был изменен с 3 на 2 (количество эпох, после которых, если не увеличается точность, то уменьшается learning rate)
    
6. Далее для возможного улучшения предсказания качества модели на валидационной выборке использовалась техника Test Time Augmentations, которая основывается на небольших изменениях данных валидационной выборки (аугментация валидационной выборки) и усреднении полученных предсказаний (небольшие изменения могут помочь модели правильно предсказать класс изображения).

Для сокращения написания кода на каждом шаге были написаны функции:  
- на создание генераторов данных и проведение аугментации;
- на сборку листа функций callbacks при обучении модели; 
- функции сохранения и вывода на экран accuracy и loss по эпохам после обучения модели для анализа качества обучения модели; 
- функция сборки модели для последующего ее обучения (сборка проводилась на каждом шаге заново, чтобы унифицировать код и ввиду того, что количество обучаемых слоев могло быть разным на различных шагах при загрузке модели для обучения с какого-либо шага).

В данное проекте представлены: 
- ноутбук с решением (car-clf-nn-2021_best_ver_to_check.ipynb); 
- результаты предсказания модели на валидационной выборке (submission_st_5_3_best_to_check.csv); 
- ссылки на модель после обучения шага 5_3 (model_step5_3.hdf5) и ее веса на шагах 5_1, 5_2, 5_3 (best_model_st_5_1.hdf5, best_model_st_5_2.hdf5, best_model_st_5_3.hdf5): https://drive.google.com/drive/folders/1myedVEqymkIYCOzOj18ChFHfSvswdRv1?usp=sharing.  
- ссылка на ноутбук с кодом на Kaggle: https://www.kaggle.com/ostrebko/car-clf-nn-2021

Что еще можно сделать для улучшения модели/доработки проекта:
1. Попробовать другие архитектуры сетей из SOTA на ImageNet позднее B6, дающие бОльшую точность, например ImageNetB7 или более точные SOTA.
2. Поэкспериментировать с архитектурой «головы» (например, добавить еще 1-2 полносвязных слоев).
3. Попробовать больше эпох на 5 этапе обучения (увеличить до 30 эпох с callback ReduceLROnPlateau с параметрами monitor='val_accuracy', factor=0.2-0.5, patience=3-5).
4. Не пробовал технику управления Learning Rate: Sheduler, но он требует большего времени и требует исключение ограничения на сессию 9 часов.
5. Использовать внешние датасеты для дообучения модели.
6. Обернуть модель в сервис на Flask (чтобы на практике отследить особенности внедрения DL-моделей в продакшн).
