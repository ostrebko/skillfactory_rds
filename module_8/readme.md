Проект "Прогнозирование стоимости автомобиля по характеристикам, описанию и фотографии".  
Задача создать модель, которая будет предсказывать стоимость автомобиля. В качестве исходных данных были табличные данные с характеристиками автомобилей, описанием продавцов авто и фотографии.  
Целью данной работы было познакомиться с возможностью построения модели с использованием разного типа данных (Multi-Input сеть).  

Для работы с моделью в первую очередь была оценена "наивная" (Модель 1) и baseline модели. Наивная модель предсказывает среднюю цену по модели автомобиля и году выпуска. Данные модели необходимы для оценки качества более сложных моделей.  

Далее был проведен анализ табличных категориальных и числовых признаков (EDA). Анализ проведен в отдельном расшаренном файле https://www.kaggle.com/ostrebko/prolect-8-batmobil-eda. Также ноутбук можно скачать в данном проекте (prolect-8-batmobil-eda.ipynb).  

Следующим шагом была проведена обработка и нормировка признаков, в чтом числе на основе EDA (см. выше). Результатом работы стало получение таблицы данных для подачи в ML и DL - модели. В приведенном ноутбуке имеется строка с неактивным кодом, чтобы показать, какие новые признаки были созданы, однако их добавление не улучшило качество модели catboost (см. далее по тексту). При работе также были написаны отдельные функции для преобразования данных, чтобы можно было в последствии заново сгенерировать признаки, если потребуется возвращение к версии без изменений.

Для оценки качества моделей использовала метрика MAPE (Mean absolute percentage error) или Средняя абсолютная ошибка в процентах (см. https://en.wikipedia.org/wiki/Mean_absolute_percentage_error). 

Модель 2: Следующим шагом было создание модели "классического"  ML с помощью CatBoost (на основе градиентного бустинга), ее тренировка, оценка результата. Лучший полученный результат на тренировочной выборке составил 12,15%, что лучше, чем результаты наивной модели и Baseline. Однако результат на валидационной выборке, по которой оценивался submit был несколько хуже и составил 13.09%. 

Модель 3 представляет собой простую нейронная сеть с Dense слоями. В отличие от Baseline был добавлен слой BatchNormalization, изменен параметр units выходов в слоях Dense и слоев Dropout. Добавление большего числа скрытых слоев качество модели не улучшило. Лучший полученный результат на тренировочной выборке составил 10,29%, что гораздо лучше, чем результаты наивной модели и Baseline. Однако результат на валидационной выборке, по которой оценивался submit был несколько, но все же был достаточно высоким и составил 10.93%.

Модель 4 представляет собой с две нейронные сети, выхода которых объединяются в единую нейронную сеть (добавляется одна "голова"). Каждая нейронная сеть (без "головы") предназначена обработки разных типов данных: табличные данные и текст. Для работы с табличнвми данными использована Model 3 (см. выше). Для работы с тектовыми данными была использована нейронная сеть с ячейками LSTM. Текст перед подачей в нейронную сеть "очищался" от знаков преписания, чисел, пробелов и повторяющихся больше 2х раз букв, а затем каждое слово приводилось в нормальную форму с помощью библиотеки pymorphy2 (MorphAnalyzer),  исключены "стоп слова" (библиотека ntlk, stopwords). Дополнительно был добавлен фильтр с исключением часто и редко встречающихся слов. Далее обработанный текст токенизировался и векторизовался для последующей подачи в нейронную сеть. При работе нейронной сетью для обработки текста также в отличие от Baseline был добавлен слой BatchNormalization, изменен параметр units выходов в слоях Dense и слоев Dropout. Добавление большего числа скрытых слоев качество модели не улучшило. Лучший полученный результат на тренировочной выборке составил 10,19% (в отдельном случае был достигнут результат 10.12%, но поторить данный результат не получилось). Rfxt что гораздо лучше, чем результаты наивной модели и Baseline. Однако результат на валидационной выборке, по которой оценивался submit был несколько, но все же был достаточно высоким и составил 10.93%.







Va) SimpleNN - Создание простой модели DL (на основе полносвязной нейронной сети), тренировка, оценка результата

Vb) NLP:

    Работа с текстом, обработка приведение в векторный вид
    NLP - Cоздание модели DL для работы с текстом (блоки LSTM, GRU, Transformer)

Vc) Создание multi-input нейронной сети (SimpleNN + NLP) для анализа табличных данных и текста одновременно, тренировка, оценка результата

Vd) EFN:

    обработка изображений с помощью библиотеки albumentations
    EFN - Cоздание модели DL для работы с изображениями на основе TransferLearning + использование техники FineTunning

Ve) Создание multi-input нейронной сети (SimpleNN + NLP + EFN) для анализа табличных данных, текста и картинок одновременно, тренировка, оценка результата

В процессе работы над шагами Vd, Ve за отведенное время, результатов лучше, чем на шаге Vc получены не были. В связи с этим на проверку итоговый вариант был отправлен без этих шагов.

VI) Ансамблирование градиентного бустинга и нейронной сети (усреднение их предсказаний)

VII) Проброс признаков
